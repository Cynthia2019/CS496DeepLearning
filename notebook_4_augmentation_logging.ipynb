{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76d05231-77c8-4d94-a445-0748a6033e48",
   "metadata": {
    "id": "76d05231-77c8-4d94-a445-0748a6033e48"
   },
   "source": [
    "# Notebook 4: Data Augmentation and Logging\n",
    "\n",
    "In this notebook, we'll expand our training loop for image classification to include __data augmentation__. We'll also use PyTorch's built-in __logging__ tools to monitor our network's progress as it trains.\n",
    "\n",
    "The notebook is broken up as follows:\n",
    "\n",
    "  1. [Setup](#setup)  \n",
    "  2. [Neural Networks for Image Recognition](#review)\n",
    "  3. [Data Augmentation](#augmentation)  \n",
    "  4. [Logging](#logging)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df0ea70-8b9e-4c8d-88f4-b8b947e24a47",
   "metadata": {
    "id": "2df0ea70-8b9e-4c8d-88f4-b8b947e24a47",
    "tags": []
   },
   "source": [
    "## __1.__ <a name=\"setup\">Setup</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ffed19-bf0e-4b4f-86e5-c13456ded4fa",
   "metadata": {
    "id": "08ffed19-bf0e-4b4f-86e5-c13456ded4fa"
   },
   "source": [
    "Make sure the needed packages are installed and utility code is in the right place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7593208-6c33-463f-82d7-6e411696f76e",
   "metadata": {
    "id": "d7593208-6c33-463f-82d7-6e411696f76e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'course-deep-learning' already exists and is not an empty directory.\n",
      "Requirement already satisfied: torch in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.10.0)\n",
      "Requirement already satisfied: torchvision in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (0.11.1)\n",
      "Requirement already satisfied: gdown>=4.4.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (4.4.0)\n",
      "Requirement already satisfied: torchaudio in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (0.10.0)\n",
      "Requirement already satisfied: librosa in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (0.8.1)\n",
      "Requirement already satisfied: matplotlib in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (3.5.1)\n",
      "Requirement already satisfied: tensorboard in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (2.8.0)\n",
      "Requirement already satisfied: ipython>=7.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (8.1.1)\n",
      "Requirement already satisfied: ipykernel in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (6.9.2)\n",
      "Requirement already satisfied: tqdm in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (4.62.3)\n",
      "Requirement already satisfied: numpy<=1.21 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 12)) (1.21.0)\n",
      "Requirement already satisfied: seaborn in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 13)) (0.11.2)\n",
      "Requirement already satisfied: torchsummary in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 14)) (1.5.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (3.10.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from torchvision->-r requirements.txt (line 2)) (8.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from gdown>=4.4.0->-r requirements.txt (line 3)) (4.10.0)\n",
      "Requirement already satisfied: filelock in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from gdown>=4.4.0->-r requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: six in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from gdown>=4.4.0->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: requests[socks] in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from gdown>=4.4.0->-r requirements.txt (line 3)) (2.26.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from librosa->-r requirements.txt (line 5)) (1.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from librosa->-r requirements.txt (line 5)) (21.2)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from librosa->-r requirements.txt (line 5)) (0.2.2)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from librosa->-r requirements.txt (line 5)) (1.0.2)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from librosa->-r requirements.txt (line 5)) (2.1.9)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from librosa->-r requirements.txt (line 5)) (0.10.3.post1)\n",
      "Requirement already satisfied: numba>=0.43.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from librosa->-r requirements.txt (line 5)) (0.55.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from librosa->-r requirements.txt (line 5)) (1.6.0)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from librosa->-r requirements.txt (line 5)) (5.1.1)\n",
      "Requirement already satisfied: joblib>=0.14 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from librosa->-r requirements.txt (line 5)) (1.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (2.4.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (4.31.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (1.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (3.20.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (3.3.6)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (1.44.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (49.2.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (0.37.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (2.6.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (2.1.1)\n",
      "Requirement already satisfied: pickleshare in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipython>=7.0->-r requirements.txt (line 8)) (0.7.5)\n",
      "Requirement already satisfied: backcall in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipython>=7.0->-r requirements.txt (line 8)) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipython>=7.0->-r requirements.txt (line 8)) (3.0.28)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipython>=7.0->-r requirements.txt (line 8)) (0.1.3)\n",
      "Requirement already satisfied: traitlets>=5 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipython>=7.0->-r requirements.txt (line 8)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipython>=7.0->-r requirements.txt (line 8)) (0.18.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipython>=7.0->-r requirements.txt (line 8)) (4.8.0)\n",
      "Requirement already satisfied: appnope in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipython>=7.0->-r requirements.txt (line 8)) (0.1.2)\n",
      "Requirement already satisfied: stack-data in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipython>=7.0->-r requirements.txt (line 8)) (0.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipython>=7.0->-r requirements.txt (line 8)) (2.11.2)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipykernel->-r requirements.txt (line 9)) (1.5.1)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipykernel->-r requirements.txt (line 9)) (6.1)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipykernel->-r requirements.txt (line 9)) (7.1.2)\n",
      "Requirement already satisfied: nest-asyncio in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipykernel->-r requirements.txt (line 9)) (1.5.4)\n",
      "Requirement already satisfied: psutil in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipykernel->-r requirements.txt (line 9)) (5.9.0)\n",
      "Requirement already satisfied: pandas>=0.23 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from seaborn->-r requirements.txt (line 13)) (1.4.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (5.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 7)) (1.3.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from jedi>=0.16->ipython>=7.0->-r requirements.txt (line 8)) (0.8.3)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel->-r requirements.txt (line 9)) (4.9.2)\n",
      "Requirement already satisfied: pyzmq>=13 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel->-r requirements.txt (line 9)) (22.3.0)\n",
      "Requirement already satisfied: entrypoints in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel->-r requirements.txt (line 9)) (0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 7)) (4.11.3)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from numba>=0.43.0->librosa->-r requirements.txt (line 5)) (0.38.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from pandas>=0.23->seaborn->-r requirements.txt (line 13)) (2021.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from pexpect>4.3->ipython>=7.0->-r requirements.txt (line 8)) (0.7.0)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from pooch>=1.0->librosa->-r requirements.txt (line 5)) (1.4.4)\n",
      "Requirement already satisfied: wcwidth in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.0->-r requirements.txt (line 8)) (0.2.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from requests[socks]->gdown>=4.4.0->-r requirements.txt (line 3)) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from requests[socks]->gdown>=4.4.0->-r requirements.txt (line 3)) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from requests[socks]->gdown>=4.4.0->-r requirements.txt (line 3)) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from requests[socks]->gdown>=4.4.0->-r requirements.txt (line 3)) (3.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa->-r requirements.txt (line 5)) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from soundfile>=0.10.2->librosa->-r requirements.txt (line 5)) (1.15.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from beautifulsoup4->gdown>=4.4.0->-r requirements.txt (line 3)) (2.3.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from requests[socks]->gdown>=4.4.0->-r requirements.txt (line 3)) (1.7.1)\n",
      "Requirement already satisfied: executing in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from stack-data->ipython>=7.0->-r requirements.txt (line 8)) (0.8.3)\n",
      "Requirement already satisfied: pure-eval in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from stack-data->ipython>=7.0->-r requirements.txt (line 8)) (0.2.2)\n",
      "Requirement already satisfied: asttokens in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from stack-data->ipython>=7.0->-r requirements.txt (line 8)) (2.0.5)\n",
      "Requirement already satisfied: pycparser in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa->-r requirements.txt (line 5)) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 7)) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 7)) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "# helper code from the course repository\n",
    "!git clone https://github.com/interactiveaudiolab/course-deep-learning.git\n",
    "# install common pacakges used for deep learning\n",
    "!cd course-deep-learning/ && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70c1cadb-a5d7-4cec-8ef7-04253a7036ba",
   "metadata": {
    "id": "70c1cadb-a5d7-4cec-8ef7-04253a7036ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yujiaxie/Desktop/northwestern/CS496DL/course-deep-learning\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "#sys.path.append('/Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages')\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "%cd course-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bf3ea7-458d-46cb-811f-e3607796845e",
   "metadata": {
    "id": "d4bf3ea7-458d-46cb-811f-e3607796845e"
   },
   "source": [
    "## __2.__ <a name=\"review\">Neural Networks for Image Recognition</a>\n",
    "\n",
    "In the previous notebook, we designed and trained a neural network to perform digit recognition on the MNIST dataset. In this notebook, we'll also consider a __convolutional neural network__ for the same task. Recall that convolutional networks use weight __kernels__ to capture correlations between neighboring coordinates. We can wrap the application of these kernels into a \"layer\" in the same way we do for weight-input dot products in a multilayer perceptron.\n",
    "\n",
    "In PyTorch, we can define a two-dimensional convolutional layer as follows:\n",
    "\n",
    "```\n",
    "conv_layer = nn.Conv2d(\n",
    "  in_channels,\n",
    "  out_channels,\n",
    "  kernel_size,\n",
    "  stride\n",
    ")\n",
    "```\n",
    "Some things to keep in mind:\n",
    "* `in_channels` refers to the number of channels in the input. In our case, because MNIST images are grayscale (1 channel), this value will be 1 for our first layer. \n",
    "* `kernel_size` can be either a tuple specifying `(kernel_height, kernel_width)` or an integer, in which case both the kernel height and width will be set to this value. Each kernel in the layer will have dimension `(in_channels, kernel_height, kernel_width)`, and will produce a single-channel feature map when applied to the input. Thus, `out_channels` refers to both the number of channels (feature maps) in the output and the number of convolutional kernels applied in the layer. \n",
    "* `stride` refers to the hop size when applying kernels, and can be either a tuple (specifying vertical and horizontal hop sizes) or an integer (in which case the same value will be used for both). \n",
    "* For an overview of more options, see the official [PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html).\n",
    "\n",
    "In addition to convolution, we'll experiment with two additional types of layers:\n",
    "* __Dropout__ randomly zeros elements of an input tensor with a given probability, ensuring that the network learns more robust and general features. In order to apply dropout at training time but _not_ at inference time, we can call `.train()` and `.eval()` on our network as usual; these will automatically set the behavior of any dropout layers in the model. For more details, see the [PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html).\n",
    "* __Max-Pooling__ can be thought of as a convolutional layer with `out_channels=in_channels`, but with the kernel dot-product operation replaced by a maximum. This can be used to \"pool\" or compress the spatial (height/width) dimensions of tensors as they pass through the network. For more details, see the [PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b94f9f-d4a2-469b-a2c9-3c2bf9fb02ea",
   "metadata": {
    "id": "39b94f9f-d4a2-469b-a2c9-3c2bf9fb02ea"
   },
   "source": [
    "#### Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3da2571-1c64-4d01-82b4-500a0bcb39f4",
   "metadata": {
    "id": "d3da2571-1c64-4d01-82b4-500a0bcb39f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters in fully-connected network: 814090\n",
      "Parameters in convolutional network: 315146\n",
      "The convolutional network has 0.39x as many parameters\n"
     ]
    }
   ],
   "source": [
    "class LinearNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"The multi-layer perceptron from our previous notebook\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # MNIST images are (1, 28, 28) (channels, width, height)\n",
    "        self.layer_1 = nn.Linear(28*28, 1024)\n",
    "        self.layer_2 = nn.Linear(1024, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size, channels, width, height = x.size()\n",
    "        x = x.view(batch_size, -1)  # create an array of flattened images with dimension (batch_size, num_pixels)\n",
    "\n",
    "        # this time, we'll use the ReLU nonlinearity at each layer  \n",
    "        x = self.relu(self.layer_1(x))\n",
    "        x = self.layer_2(x)  # we'll avoid \"squashing\" our final outputs by omitting the sigmoid\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple convolutional neural network for image classification.\n",
    "    From https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "      # convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "\n",
    "      # just like in our fully-connected network, we'll use ReLU activations\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "      # random dropout with two different \"strengths\"\n",
    "        self.dropout1 = nn.Dropout(0.25)  # we pass the dropout probability\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "      # max-pooling\n",
    "        self.pool = nn.MaxPool2d(4)\n",
    "\n",
    "      # a final fully-connected network to map our learned convolutional\n",
    "      # features to class predictions\n",
    "        self.fc1 = nn.Linear(64*6*6, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "      # inputs are expected to have shape (batch_size, 1, 28, 28)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "      # out first convolutional layer reshapes inputs to (batch_size, 32, 26, 26)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "      # our second convolutional layer reshapes inputs to (batch_size, 64, 24, 24)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "      # our pooling layer reduces inputs to (batch_size, 64, 6, 6)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "      # we \"flatten\" inputs to (batch_size, 64 * 6 * 6) before passing to a \n",
    "      # small fully-connected network\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "      # our final outputs are vectors of class scores, with shape (batch_size, 10)\n",
    "        return x\n",
    "\n",
    "\n",
    "def param_count(m: nn.Module):\n",
    "  \"\"\"Count the number of trainable parameters (weights) in a model\"\"\"\n",
    "  return sum([p.shape.numel() for p in m.parameters() if p.requires_grad])\n",
    "\n",
    "\n",
    "model1 = LinearNetwork()\n",
    "model2 = ConvNetwork()\n",
    "\n",
    "params1 = param_count(model1)\n",
    "params2 = param_count(model2)\n",
    "\n",
    "print(f'Parameters in fully-connected network: {params1}')\n",
    "print(f'Parameters in convolutional network: {params2}')\n",
    "print(f'The convolutional network has {params2/params1 :0.2f}x as many parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d2928-7e46-4fba-8940-1240da5bf879",
   "metadata": {
    "id": "801d2928-7e46-4fba-8940-1240da5bf879"
   },
   "source": [
    "#### Training Loop\n",
    "\n",
    "Next, we'll slightly modify our training loop to allow for different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c72e3a3a-4d41-4931-90ff-1ae9cfecbe91",
   "metadata": {
    "id": "c72e3a3a-4d41-4931-90ff-1ae9cfecbe91"
   },
   "outputs": [],
   "source": [
    "def training_loop(save_path, epochs, batch_size, device=\"cpu\", use_conv=False):\n",
    "    \"\"\"\n",
    "    Train a neural network model for digit recognition on the MNIST dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    save_path (str):  path/filename for model checkpoint, e.g. 'my_model.pt'\n",
    "    \n",
    "    epochs (int):     number of iterations through the whole dataset for training\n",
    "    \n",
    "    batch_size (int): size of a single batch of inputs\n",
    "    \n",
    "    device (str):     device on which tensors are placed; should be 'cpu' or 'cuda'. \n",
    "\n",
    "    use_conv (bool):  if True, use ConvNetwork; else, use LinearNetwork.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model (nn.Module): final trained model\n",
    "    \n",
    "    save_path (str):   path/filename for model checkpoint, so that we can load our model\n",
    "                       later to test on unseen data\n",
    "    \n",
    "    device (str):      the device on which we carried out training, so we can match it\n",
    "                       when we test the final model on unseen data later\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize model\n",
    "    if use_conv:\n",
    "      model = ConvNetwork()\n",
    "      print('Training convolutional neural network...')\n",
    "    else:\n",
    "      model = LinearNetwork()\n",
    "      print('Training fully-connected neural network...')\n",
    "\n",
    "    print(f'Parameters in model: {param_count(model)}')\n",
    "    model.to(device)\n",
    "\n",
    "    # initialize an optimizer to update our model's parameters during training\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    # optimizer = torch.optim.Adadelta(model.parameters(), lr=1.0)\n",
    "\n",
    "    # make a new directory in which to download the MNIST dataset\n",
    "    data_dir = \"./data/\"\n",
    "    \n",
    "    # initialize a Transform object to prepare our data\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        lambda x: x>0,\n",
    "        lambda x: x.float(),\n",
    "    ])\n",
    "\n",
    "    # load MNIST \"test\" dataset from disk\n",
    "    mnist_test = datasets.MNIST(data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "    # load MNIST \"train\" dataset from disk and set aside a portion for validation\n",
    "    mnist_train_full = datasets.MNIST(data_dir, train=True, download=True, transform=transform)\n",
    "    mnist_train, mnist_val = torch.utils.data.random_split(mnist_train_full, [55000, 5000])\n",
    "\n",
    "    # initialize a DataLoader object for each dataset\n",
    "    train_dataloader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(mnist_val, batch_size=batch_size, shuffle=False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(mnist_test, batch_size=1, shuffle=False)\n",
    "\n",
    "    # a PyTorch categorical cross-entropy loss object\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # time training process\n",
    "    st = time.time()\n",
    "\n",
    "    # keep track of best validation accuracy; if improved upon, save checkpoint\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # time to start training!\n",
    "    for epoch_idx, epoch in enumerate(range(epochs)):\n",
    "\n",
    "        # loop through the entire dataset once per epoch\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        train_total = 0\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # unpack data and labels\n",
    "            x, y = batch\n",
    "            x = x.to(device)  # we'll cover this in the next section!\n",
    "            y = y.to(device)  # we'll cover this in the next section!\n",
    "\n",
    "            # generate predictions and compute loss\n",
    "            output = model(x)  # (batch_size, 10)\n",
    "            loss = loss_fn(output, y)\n",
    "\n",
    "            # compute accuracy\n",
    "            preds = output.argmax(dim=1)\n",
    "            acc = preds.eq(y).sum().item()/len(y)\n",
    "\n",
    "            # compute gradients and update model parameters\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update statistics\n",
    "            train_loss += (loss * len(x))\n",
    "            train_acc += (acc * len(x))\n",
    "            train_total += len(x)\n",
    "\n",
    "        train_loss /= train_total\n",
    "        train_acc /= train_total\n",
    "\n",
    "        # perform validation once per epoch\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        val_total = 0\n",
    "        model.eval()\n",
    "        for batch_idx, batch in enumerate(val_dataloader):\n",
    "\n",
    "            # don't compute gradients during validation\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # unpack data and labels\n",
    "                x, y = batch\n",
    "                x = x.to(device)  # we'll cover this in the next section!\n",
    "                y = y.to(device)  # we'll cover this in the next section!\n",
    "\n",
    "                # generate predictions and compute loss\n",
    "                output = model(x)\n",
    "                loss = loss_fn(output, y)\n",
    "\n",
    "                # compute accuracy\n",
    "                preds = output.argmax(dim=1)\n",
    "                acc = preds.eq(y).sum().item()/len(y)\n",
    "\n",
    "                # update statistics\n",
    "                val_loss += (loss * len(x))\n",
    "                val_acc += (acc * len(x))\n",
    "                val_total += len(x)\n",
    "\n",
    "        val_loss /= val_total\n",
    "        val_acc /= val_total\n",
    "        print(f\"Epoch {epoch_idx + 1}: val loss {val_loss :0.3f}, val acc {val_acc :0.3f}, train loss {train_loss :0.3f}, train acc {train_acc :0.3f}\")\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            print(f\"New best accuracy {val_acc : 0.3f} (old {best_acc : 0.3f}); saving model weights to {save_path}\")\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    print(f\"Total training time (s): {time.time() - st :0.3f}\")\n",
    "    \n",
    "    return model, save_path, device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9367db63-7e64-4ac8-9216-be729f9c15bf",
   "metadata": {
    "id": "9367db63-7e64-4ac8-9216-be729f9c15bf"
   },
   "source": [
    "#### Run It!\n",
    "\n",
    "Finally, we can compare our convolutional and fully-connected models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9418055-6cf2-43e9-8d6f-5c6e50735f09",
   "metadata": {
    "id": "b9418055-6cf2-43e9-8d6f-5c6e50735f09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training convolutional neural network...\n",
      "Parameters in model: 315146\n",
      "Epoch 1: val loss 0.470, val acc 0.858, train loss 1.271, train acc 0.582\n",
      "New best accuracy  0.858 (old  0.000); saving model weights to mnist_cnn.pt\n",
      "Epoch 2: val loss 0.272, val acc 0.918, train loss 0.504, train acc 0.840\n",
      "New best accuracy  0.918 (old  0.858); saving model weights to mnist_cnn.pt\n",
      "Epoch 3: val loss 0.189, val acc 0.942, train loss 0.331, train acc 0.898\n",
      "New best accuracy  0.942 (old  0.918); saving model weights to mnist_cnn.pt\n",
      "Epoch 4: val loss 0.147, val acc 0.960, train loss 0.259, train acc 0.923\n",
      "New best accuracy  0.960 (old  0.942); saving model weights to mnist_cnn.pt\n",
      "Epoch 5: val loss 0.129, val acc 0.963, train loss 0.213, train acc 0.936\n",
      "New best accuracy  0.963 (old  0.960); saving model weights to mnist_cnn.pt\n",
      "Epoch 6: val loss 0.110, val acc 0.966, train loss 0.192, train acc 0.942\n",
      "New best accuracy  0.966 (old  0.963); saving model weights to mnist_cnn.pt\n",
      "Epoch 7: val loss 0.101, val acc 0.970, train loss 0.173, train acc 0.947\n",
      "New best accuracy  0.970 (old  0.966); saving model weights to mnist_cnn.pt\n",
      "Epoch 8: val loss 0.092, val acc 0.973, train loss 0.161, train acc 0.951\n",
      "New best accuracy  0.973 (old  0.970); saving model weights to mnist_cnn.pt\n",
      "Epoch 9: val loss 0.085, val acc 0.975, train loss 0.148, train acc 0.956\n",
      "New best accuracy  0.975 (old  0.973); saving model weights to mnist_cnn.pt\n",
      "Epoch 10: val loss 0.080, val acc 0.977, train loss 0.138, train acc 0.959\n",
      "New best accuracy  0.977 (old  0.975); saving model weights to mnist_cnn.pt\n",
      "Epoch 11: val loss 0.077, val acc 0.979, train loss 0.134, train acc 0.959\n",
      "New best accuracy  0.979 (old  0.977); saving model weights to mnist_cnn.pt\n",
      "Epoch 12: val loss 0.074, val acc 0.979, train loss 0.127, train acc 0.962\n",
      "Epoch 13: val loss 0.072, val acc 0.978, train loss 0.120, train acc 0.964\n",
      "Epoch 14: val loss 0.067, val acc 0.982, train loss 0.116, train acc 0.964\n",
      "New best accuracy  0.982 (old  0.979); saving model weights to mnist_cnn.pt\n",
      "Epoch 15: val loss 0.063, val acc 0.982, train loss 0.113, train acc 0.967\n",
      "New best accuracy  0.982 (old  0.982); saving model weights to mnist_cnn.pt\n",
      "Epoch 16: val loss 0.066, val acc 0.980, train loss 0.108, train acc 0.967\n",
      "Epoch 17: val loss 0.065, val acc 0.981, train loss 0.105, train acc 0.968\n",
      "Epoch 18: val loss 0.061, val acc 0.983, train loss 0.101, train acc 0.969\n",
      "New best accuracy  0.983 (old  0.982); saving model weights to mnist_cnn.pt\n",
      "Epoch 19: val loss 0.058, val acc 0.984, train loss 0.097, train acc 0.970\n",
      "New best accuracy  0.984 (old  0.983); saving model weights to mnist_cnn.pt\n",
      "Epoch 20: val loss 0.059, val acc 0.983, train loss 0.095, train acc 0.971\n",
      "Total training time (s): 1550.671\n",
      "Training fully-connected neural network...\n",
      "Parameters in model: 814090\n",
      "Epoch 1: val loss 0.471, val acc 0.875, train loss 0.962, train acc 0.801\n",
      "New best accuracy  0.875 (old  0.000); saving model weights to mnist_fc.pt\n",
      "Epoch 2: val loss 0.361, val acc 0.897, train loss 0.419, train acc 0.884\n",
      "New best accuracy  0.897 (old  0.875); saving model weights to mnist_fc.pt\n",
      "Epoch 3: val loss 0.322, val acc 0.905, train loss 0.355, train acc 0.899\n",
      "New best accuracy  0.905 (old  0.897); saving model weights to mnist_fc.pt\n",
      "Epoch 4: val loss 0.299, val acc 0.911, train loss 0.322, train acc 0.907\n",
      "New best accuracy  0.911 (old  0.905); saving model weights to mnist_fc.pt\n",
      "Epoch 5: val loss 0.279, val acc 0.916, train loss 0.300, train acc 0.913\n",
      "New best accuracy  0.916 (old  0.911); saving model weights to mnist_fc.pt\n",
      "Epoch 6: val loss 0.265, val acc 0.918, train loss 0.281, train acc 0.919\n",
      "New best accuracy  0.918 (old  0.916); saving model weights to mnist_fc.pt\n",
      "Epoch 7: val loss 0.253, val acc 0.924, train loss 0.265, train acc 0.923\n",
      "New best accuracy  0.924 (old  0.918); saving model weights to mnist_fc.pt\n",
      "Epoch 8: val loss 0.239, val acc 0.929, train loss 0.251, train acc 0.927\n",
      "New best accuracy  0.929 (old  0.924); saving model weights to mnist_fc.pt\n",
      "Epoch 9: val loss 0.230, val acc 0.930, train loss 0.239, train acc 0.931\n",
      "New best accuracy  0.930 (old  0.929); saving model weights to mnist_fc.pt\n",
      "Epoch 10: val loss 0.220, val acc 0.932, train loss 0.227, train acc 0.935\n",
      "New best accuracy  0.932 (old  0.930); saving model weights to mnist_fc.pt\n",
      "Epoch 11: val loss 0.213, val acc 0.935, train loss 0.217, train acc 0.938\n",
      "New best accuracy  0.935 (old  0.932); saving model weights to mnist_fc.pt\n",
      "Epoch 12: val loss 0.204, val acc 0.936, train loss 0.208, train acc 0.940\n",
      "New best accuracy  0.936 (old  0.935); saving model weights to mnist_fc.pt\n",
      "Epoch 13: val loss 0.196, val acc 0.942, train loss 0.199, train acc 0.943\n",
      "New best accuracy  0.942 (old  0.936); saving model weights to mnist_fc.pt\n",
      "Epoch 14: val loss 0.189, val acc 0.944, train loss 0.191, train acc 0.945\n",
      "New best accuracy  0.944 (old  0.942); saving model weights to mnist_fc.pt\n",
      "Epoch 15: val loss 0.183, val acc 0.946, train loss 0.183, train acc 0.947\n",
      "New best accuracy  0.946 (old  0.944); saving model weights to mnist_fc.pt\n",
      "Epoch 16: val loss 0.180, val acc 0.947, train loss 0.177, train acc 0.949\n",
      "New best accuracy  0.947 (old  0.946); saving model weights to mnist_fc.pt\n",
      "Epoch 17: val loss 0.174, val acc 0.949, train loss 0.170, train acc 0.950\n",
      "New best accuracy  0.949 (old  0.947); saving model weights to mnist_fc.pt\n",
      "Epoch 18: val loss 0.169, val acc 0.951, train loss 0.164, train acc 0.953\n",
      "New best accuracy  0.951 (old  0.949); saving model weights to mnist_fc.pt\n",
      "Epoch 19: val loss 0.163, val acc 0.952, train loss 0.158, train acc 0.954\n",
      "New best accuracy  0.952 (old  0.951); saving model weights to mnist_fc.pt\n",
      "Epoch 20: val loss 0.160, val acc 0.953, train loss 0.153, train acc 0.956\n",
      "New best accuracy  0.953 (old  0.952); saving model weights to mnist_fc.pt\n",
      "Total training time (s): 208.341\n"
     ]
    }
   ],
   "source": [
    "# train a convolutional neural network\n",
    "conv_model, conv_path, device = training_loop(\n",
    "    save_path=\"mnist_cnn.pt\", \n",
    "    epochs=20, \n",
    "    batch_size=60, \n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    use_conv=True\n",
    ")\n",
    "\n",
    "# train a fully-connected neural network\n",
    "lin_model, lin_path, device = training_loop(\n",
    "    save_path=\"mnist_fc.pt\", \n",
    "    epochs=20, \n",
    "    batch_size=60, \n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    use_conv=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZMAkPGvztiBU",
   "metadata": {
    "id": "ZMAkPGvztiBU"
   },
   "source": [
    "Our convolutional network is able to achieve a classification accuracy __~4%__ higher than our fully-connected network, with less than half the parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbb51e2-bcdc-435c-aea8-f2efa94df648",
   "metadata": {
    "id": "cfbb51e2-bcdc-435c-aea8-f2efa94df648",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e",
    "tags": []
   },
   "source": [
    "## __3.__ <a name=\"augmentation\">Data Augmentation</a>\n",
    "\n",
    "We've got a pretty accurate model, but there are plenty of deep learning tricks we can use to squeeze some extra performance. One common practice is __data augmentation__, in which random transformations are applied to inputs during training. This helps in two ways:\n",
    "* Often, datasets are relatively small and imperfectly represent the popluation from which they are sampled. Data augmentation effectively expands the size of the dataset through sampling additional randomized variations of each instance.\n",
    "* We typically want to train a model that is __robust__ against common real-world transformations of its inputs -- that is, a model whose predictions are __invariant__ under these transformations. Data augmentation exposes our model to a chosen set of transformations during training so that it can learn to \"see past\" them.\n",
    "\n",
    "TorchVision provides a number of `Transform` objects designed to perform data augmentation, making it easy to apply transformations automatically when data is fetched from a `Dataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3663caa-62a9-4ecd-803d-b2bd36c6a231",
   "metadata": {
    "id": "f3663caa-62a9-4ecd-803d-b2bd36c6a231"
   },
   "outputs": [],
   "source": [
    "# directory for MNIST dataset\n",
    "data_dir = \"./data/\"\n",
    "\n",
    "# initialize a Transform object to prepare our data\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    lambda x: x>0,\n",
    "    lambda x: x.float(),\n",
    "])\n",
    "\n",
    "# load MNIST \"train\" dataset from disk\n",
    "mnist_train = datasets.MNIST(data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "# fetch an image from the MNIST dataset\n",
    "example_img, example_label = mnist_train[300]\n",
    "plt.imshow(example_img.squeeze(), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# perform a random affine transformation of an input (rotation, translation, shear)\n",
    "affine_aug = torchvision.transforms.RandomAffine(degrees=(-30, 30), translate=(0.25, 0.25), shear=(-45, 45))\n",
    "augmented = affine_aug(example_img)\n",
    "plt.imshow(augmented.squeeze(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9650113-cdf9-4b1f-a39e-5863287d0ca2",
   "metadata": {
    "id": "d9650113-cdf9-4b1f-a39e-5863287d0ca2",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "Because we're effectively increasing the size of the dataset, and due to the computation required to perform each transformation, training with data augmentation may take more time (as measured in both walltime and iterations). It's also worth noting that augmentations are typically applied to the training data only. While we won't go into detail at the moment, feel free to try training with any of the [augmentations offered by TorchVision](https://pytorch.org/vision/stable/transforms.html). You can add augmentations to the training loop above by editing the `transfom` object:\n",
    "\n",
    "```\n",
    "# initialize a Transform object to prepare our data\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    lambda x: x>0,\n",
    "    lambda x: x.float(),\n",
    "    torchvision.transforms.RandomAffine(degrees=(-30, 30), translate=(0.25, 0.25), shear=(-45, 45))  # just append transforms!\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d616308-8574-43bb-a9b3-ea3b92f899e9",
   "metadata": {
    "id": "6d616308-8574-43bb-a9b3-ea3b92f899e9",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e",
    "tags": []
   },
   "source": [
    "## __4.__ <a name=\"logging\">Logging</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9fe057-59da-47a9-9ccf-ca9d96070ca1",
   "metadata": {
    "id": "7f9fe057-59da-47a9-9ccf-ca9d96070ca1",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "In our training loop, we print running summaries of our model's training performance in order to monitor its progress. This is somewhat clunky and limited - what if we want to plot accuracy in real time, visualize challenging instances, dynamically change what information is displayed, or document and compare across multiple training runs? All these tasks fall under the umbrella of __logging__, and once again, PyTorch provides utilities to simplify the process. We can use PyTorch's built-in TensorBoard support to configure and view training logs without the need for any external database or visualization software. To launch TensorBoard within the notebook, run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfc97e7-86a5-41c7-941d-951dce68a654",
   "metadata": {
    "id": "4bfc97e7-86a5-41c7-941d-951dce68a654"
   },
   "outputs": [],
   "source": [
    "# here, we'll initialize TensorBoard. You should see an empty window in this cell, which will populate with\n",
    "# graphs as soon as we run our training code below.\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a8b591-8607-437f-9380-2a99c3cf1e2d",
   "metadata": {
    "id": "b0a8b591-8607-437f-9380-2a99c3cf1e2d",
    "outputId": "b47b6db9-4f27-42f0-bb93-f497018aa513"
   },
   "source": [
    "Next, we'll re-write out training loop to log loss and accuracy values to TensorBoard rather than printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633848e-400c-4e8c-a73b-04b4244b2377",
   "metadata": {
    "id": "7633848e-400c-4e8c-a73b-04b4244b2377"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pathlib import Path\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# save all log data to a local directory\n",
    "run_dir = \"logs\"\n",
    "\n",
    "# timestamp the logs for each run so we can sort through them \n",
    "run_time = datetime.datetime.now().strftime(\"%I:%M%p on %B %d, %Y\")\n",
    "\n",
    "# initialize a SummaryWriter object to handle all logging actions\n",
    "logger = SummaryWriter(log_dir=Path(run_dir) / run_time)\n",
    "\n",
    "def training_loop(save_path, \n",
    "                  epochs, \n",
    "                  batch_size, \n",
    "                  device=\"cpu\", \n",
    "                  use_conv=False,\n",
    "                  logger=None\n",
    "                  ):\n",
    "    \"\"\"\n",
    "    Train a neural network model for digit recognition on the MNIST dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    save_path (str):        path/filename for model checkpoint, e.g. 'my_model.pt'\n",
    "    \n",
    "    epochs (int):           number of iterations through the whole dataset for training\n",
    "    \n",
    "    batch_size (int):       size of a single batch of inputs\n",
    "    \n",
    "    device (str):           device on which tensors are placed; should be 'cpu' or 'cuda'. \n",
    "\n",
    "    use_conv (bool):        if True, use ConvNetwork; else, use LinearNetwork.\n",
    "\n",
    "    logger (SummaryWriter): a TensorBoard logger\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model (nn.Module): final trained model\n",
    "    \n",
    "    save_path (str):   path/filename for model checkpoint, so that we can load our model\n",
    "                       later to test on unseen data\n",
    "    \n",
    "    device (str):      the device on which we carried out training, so we can match it\n",
    "                       when we test the final model on unseen data later\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize model\n",
    "    if use_conv:\n",
    "      model = ConvNetwork()\n",
    "      print('Training convolutional neural network...')\n",
    "    else:\n",
    "      model = LinearNetwork()\n",
    "      print('Training fully-connected neural network...')\n",
    "\n",
    "    print(f'Parameters in model: {param_count(model)}')\n",
    "    model.to(device)\n",
    "\n",
    "    # initialize an optimizer to update our model's parameters during training\n",
    "    if use_conv:\n",
    "      optimizer = torch.optim.Adadelta(model.parameters(), lr=1.0)\n",
    "    else:\n",
    "      optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # make a new directory in which to download the MNIST dataset\n",
    "    data_dir = \"./data/\"\n",
    "    \n",
    "    # initialize a Transform object to prepare our data\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        lambda x: x>0,\n",
    "        lambda x: x.float(),\n",
    "    ])\n",
    "\n",
    "    # load MNIST \"test\" dataset from disk\n",
    "    mnist_test = datasets.MNIST(data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "    # load MNIST \"train\" dataset from disk and set aside a portion for validation\n",
    "    mnist_train_full = datasets.MNIST(data_dir, train=True, download=True, transform=transform)\n",
    "    mnist_train, mnist_val = torch.utils.data.random_split(mnist_train_full, [55000, 5000])\n",
    "\n",
    "    # initialize a DataLoader object for each dataset\n",
    "    train_dataloader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(mnist_val, batch_size=batch_size, shuffle=False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(mnist_test, batch_size=1, shuffle=False)\n",
    "\n",
    "    # a PyTorch categorical cross-entropy loss object\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # time training process\n",
    "    st = time.time()\n",
    "\n",
    "    # keep track of best validation accuracy; if improved upon, save checkpoint\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # time to start training!\n",
    "    for epoch_idx, epoch in enumerate(range(epochs)):\n",
    "\n",
    "        # loop through the entire dataset once per epoch\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        train_total = 0\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # unpack data and labels\n",
    "            x, y = batch\n",
    "            x = x.to(device)  # we'll cover this in the next section!\n",
    "            y = y.to(device)  # we'll cover this in the next section!\n",
    "\n",
    "            # generate predictions and compute loss\n",
    "            output = model(x)  # (batch_size, 10)\n",
    "            loss = loss_fn(output, y)\n",
    "\n",
    "            # compute accuracy\n",
    "            preds = output.argmax(dim=1)\n",
    "            acc = preds.eq(y).sum().item()/len(y)\n",
    "\n",
    "            # compute gradients and update model parameters\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update statistics\n",
    "            train_loss += (loss * len(x))\n",
    "            train_acc += (acc * len(x))\n",
    "            train_total += len(x)\n",
    "\n",
    "        train_loss /= train_total\n",
    "        train_acc /= train_total\n",
    "\n",
    "        ########################################################################\n",
    "        # NEW: log to TensorBoard\n",
    "        ########################################################################\n",
    "\n",
    "        if logger is not None:\n",
    "          logger.add_scalar(\"train_loss\", train_loss, epoch_idx)\n",
    "          logger.add_scalar(\"train_acc\", train_acc, epoch_idx)\n",
    "\n",
    "        # perform validation once per epoch\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        val_total = 0\n",
    "        model.eval()\n",
    "        for batch_idx, batch in enumerate(val_dataloader):\n",
    "\n",
    "            # don't compute gradients during validation\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # unpack data and labels\n",
    "                x, y = batch\n",
    "                x = x.to(device)  # we'll cover this in the next section!\n",
    "                y = y.to(device)  # we'll cover this in the next section!\n",
    "\n",
    "                # generate predictions and compute loss\n",
    "                output = model(x)\n",
    "                loss = loss_fn(output, y)\n",
    "\n",
    "                # compute accuracy\n",
    "                preds = output.argmax(dim=1)\n",
    "                acc = preds.eq(y).sum().item()/len(y)\n",
    "\n",
    "                # update statistics\n",
    "                val_loss += (loss * len(x))\n",
    "                val_acc += (acc * len(x))\n",
    "                val_total += len(x)\n",
    "\n",
    "        val_loss /= val_total\n",
    "        val_acc /= val_total\n",
    "\n",
    "        ########################################################################\n",
    "        # NEW: log to TensorBoard\n",
    "        ########################################################################\n",
    "        \n",
    "        if logger is not None:\n",
    "          logger.add_scalar(\"val_loss\", val_loss, epoch_idx)\n",
    "          logger.add_scalar(\"val_acc\", val_acc, epoch_idx)\n",
    "        \n",
    "        print(f\"Epoch {epoch_idx + 1}: val loss {val_loss :0.3f}, val acc {val_acc :0.3f}, train loss {train_loss :0.3f}, train acc {train_acc :0.3f}\")\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            print(f\"New best accuracy {val_acc : 0.3f} (old {best_acc : 0.3f}); saving model weights to {save_path}\")\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    print(f\"Total training time (s): {time.time() - st :0.3f}\")\n",
    "    \n",
    "    return model, save_path, device\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c2f18-ff04-420c-a47c-16c1387bcae3",
   "metadata": {
    "id": "bb3c2f18-ff04-420c-a47c-16c1387bcae3"
   },
   "outputs": [],
   "source": [
    "# run our training loop\n",
    "model, save_path, device = training_loop(\n",
    "    save_path=\"mnist_review.pt\", \n",
    "    epochs=10, \n",
    "    batch_size=60, \n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    use_conv=True,\n",
    "    logger=logger\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb29bfa-c022-460d-92b2-706c50061272",
   "metadata": {
    "id": "dcb29bfa-c022-460d-92b2-706c50061272",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "We can also run TensorBoard from the terminal, in which case we can view the logs in a browser by navigating to the correct port on our `localhost`. In the example below, after running the command we would need to point our browser to `localhost:9999`\n",
    "\n",
    "```\n",
    "$ tensorboard --logdir /path/to/logging/directory/ --port 9999\n",
    "```\n",
    "\n",
    "If no port is given, TensorBoard will default to 6006. In fact, the logs from your experiment above should already be visible at `localhost:6006`. TensorBoard will continue serving on this port until the notebook kernel shuts down or you halt the terminal command (e.g. using `ctrl` + `c`), at which point you will not be able to view your logs until you re-start TensorBoard."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_4_augmentation_logging (2).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
