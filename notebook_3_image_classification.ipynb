{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b3745d-0dfd-4661-bece-dddb094c98e2",
   "metadata": {
    "id": "59b3745d-0dfd-4661-bece-dddb094c98e2"
   },
   "source": [
    "# Notebook 3: Training a Neural Network for Image Classification\n",
    "\n",
    "In this notebook, we'll train a neural network to classify images rather than two-dimensional synthetic data. We'll also take a look at the components of a typical training pipeline in PyTorch, including datasets, data loaders, and checkpointing. We'll use a new loss function to guide our network during training, and let one of PyTorch's optimizers automatically update our network's parameters using gradient descent.\n",
    "\n",
    "The notebook is broken up as follows:\n",
    "\n",
    "  1. [Setup](#setup)  \n",
    "  2. [Data](#data)  \n",
    "     2.1 [Datasets](#datasets)  \n",
    "     2.2 [DataLoaders](#dataloaders)  \n",
    "  3. [A Neural Network for Image Recognition](#nn)  \n",
    "     3.1. [Defining the Network](#definition)  \n",
    "     3.2  [Classification Loss](#loss)  \n",
    "     3.3  [Picking an Optimizer: SGD](#sgd)  \n",
    "     3.4. [Checkpointing](#checkpoint)  \n",
    "  4. [Putting It All Together: Training Loop](#train)  \n",
    "  5. [GPU Acceleration](#gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d22747f-31e8-4263-9e74-c499f25416b7",
   "metadata": {
    "id": "1d22747f-31e8-4263-9e74-c499f25416b7",
    "tags": []
   },
   "source": [
    "## __1.__ <a name=\"setup\">Setup</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5784a0af-7266-4bcd-b87a-2558b15da621",
   "metadata": {
    "id": "5784a0af-7266-4bcd-b87a-2558b15da621"
   },
   "source": [
    "Make sure the needed packages are installed and utility code is in the right place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27d077ec-0fd2-4c9e-9074-fd4524ead6c8",
   "metadata": {
    "id": "27d077ec-0fd2-4c9e-9074-fd4524ead6c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'course-deep-learning' already exists and is not an empty directory.\n",
      "Requirement already satisfied: torch in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.10.0)\n",
      "Requirement already satisfied: torchvision in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (0.11.1)\n",
      "Requirement already satisfied: gdown>=4.4.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (4.4.0)\n",
      "Requirement already satisfied: torchaudio in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (0.10.0)\n",
      "Requirement already satisfied: librosa in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (0.8.1)\n",
      "Requirement already satisfied: matplotlib in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (3.5.1)\n",
      "Requirement already satisfied: tensorboard in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (2.8.0)\n",
      "Requirement already satisfied: ipython>=7.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (8.1.1)\n",
      "Requirement already satisfied: ipykernel in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (6.9.2)\n",
      "Requirement already satisfied: tqdm in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (4.62.3)\n",
      "Requirement already satisfied: numpy<=1.21 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 12)) (1.21.0)\n",
      "Requirement already satisfied: seaborn in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 13)) (0.11.2)\n",
      "Requirement already satisfied: torchsummary in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from -r requirements.txt (line 14)) (1.5.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (3.10.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from torchvision->-r requirements.txt (line 2)) (8.4.0)\n",
      "Requirement already satisfied: filelock in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from gdown>=4.4.0->-r requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from gdown>=4.4.0->-r requirements.txt (line 3)) (4.10.0)\n",
      "Requirement already satisfied: requests[socks] in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from gdown>=4.4.0->-r requirements.txt (line 3)) (2.26.0)\n",
      "Requirement already satisfied: six in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from gdown>=4.4.0->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: numba>=0.43.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from librosa->-r requirements.txt (line 5)) (0.55.1)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from librosa->-r requirements.txt (line 5)) (0.10.3.post1)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from librosa->-r requirements.txt (line 5)) (2.1.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from librosa->-r requirements.txt (line 5)) (21.2)\n",
      "Requirement already satisfied: pooch>=1.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from librosa->-r requirements.txt (line 5)) (1.6.0)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from librosa->-r requirements.txt (line 5)) (0.2.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from librosa->-r requirements.txt (line 5)) (1.8.0)\n",
      "Requirement already satisfied: joblib>=0.14 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from librosa->-r requirements.txt (line 5)) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from librosa->-r requirements.txt (line 5)) (1.0.2)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from librosa->-r requirements.txt (line 5)) (5.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (4.31.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (0.11.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (1.44.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (2.6.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (49.2.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (0.37.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (3.20.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (3.3.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (2.1.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (1.0.0)\n",
      "Requirement already satisfied: pickleshare in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipython>=7.0->-r requirements.txt (line 8)) (0.7.5)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipython>=7.0->-r requirements.txt (line 8)) (0.1.3)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipython>=7.0->-r requirements.txt (line 8)) (0.18.1)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipython>=7.0->-r requirements.txt (line 8)) (2.11.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipython>=7.0->-r requirements.txt (line 8)) (3.0.28)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipython>=7.0->-r requirements.txt (line 8)) (4.8.0)\n",
      "Requirement already satisfied: appnope in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipython>=7.0->-r requirements.txt (line 8)) (0.1.2)\n",
      "Requirement already satisfied: stack-data in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipython>=7.0->-r requirements.txt (line 8)) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipython>=7.0->-r requirements.txt (line 8)) (5.1.1)\n",
      "Requirement already satisfied: backcall in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipython>=7.0->-r requirements.txt (line 8)) (0.2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter-client<8.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipykernel->-r requirements.txt (line 9)) (7.1.2)\n",
      "Requirement already satisfied: psutil in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipykernel->-r requirements.txt (line 9)) (5.9.0)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipykernel->-r requirements.txt (line 9)) (1.5.1)\n",
      "Requirement already satisfied: nest-asyncio in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipykernel->-r requirements.txt (line 9)) (1.5.4)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from ipykernel->-r requirements.txt (line 9)) (6.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from seaborn->-r requirements.txt (line 13)) (1.4.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 7)) (1.3.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from jedi>=0.16->ipython>=7.0->-r requirements.txt (line 8)) (0.8.3)\n",
      "Requirement already satisfied: entrypoints in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel->-r requirements.txt (line 9)) (0.4)\n",
      "Requirement already satisfied: pyzmq>=13 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel->-r requirements.txt (line 9)) (22.3.0)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel->-r requirements.txt (line 9)) (4.9.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 7)) (4.11.3)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from numba>=0.43.0->librosa->-r requirements.txt (line 5)) (0.38.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from pandas>=0.23->seaborn->-r requirements.txt (line 13)) (2021.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from pexpect>4.3->ipython>=7.0->-r requirements.txt (line 8)) (0.7.0)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from pooch>=1.0->librosa->-r requirements.txt (line 5)) (1.4.4)\n",
      "Requirement already satisfied: wcwidth in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.0->-r requirements.txt (line 8)) (0.2.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from requests[socks]->gdown>=4.4.0->-r requirements.txt (line 3)) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from requests[socks]->gdown>=4.4.0->-r requirements.txt (line 3)) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from requests[socks]->gdown>=4.4.0->-r requirements.txt (line 3)) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from requests[socks]->gdown>=4.4.0->-r requirements.txt (line 3)) (3.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa->-r requirements.txt (line 5)) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from soundfile>=0.10.2->librosa->-r requirements.txt (line 5)) (1.15.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from beautifulsoup4->gdown>=4.4.0->-r requirements.txt (line 3)) (2.3.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from requests[socks]->gdown>=4.4.0->-r requirements.txt (line 3)) (1.7.1)\n",
      "Requirement already satisfied: asttokens in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from stack-data->ipython>=7.0->-r requirements.txt (line 8)) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from stack-data->ipython>=7.0->-r requirements.txt (line 8)) (0.2.2)\n",
      "Requirement already satisfied: executing in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from stack-data->ipython>=7.0->-r requirements.txt (line 8)) (0.8.3)\n",
      "Requirement already satisfied: pycparser in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa->-r requirements.txt (line 5)) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 7)) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 7)) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "# helper code from the course repository\n",
    "!git clone https://github.com/interactiveaudiolab/course-deep-learning.git\n",
    "# install common pacakges used for deep learning\n",
    "!cd course-deep-learning/ && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bde4fe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.append('/Users/yujiaxie/.pyenv/versions/3.9.4/lib/python3.9/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b554b46-be39-4431-8031-b4e41d2d15b2",
   "metadata": {
    "id": "1b554b46-be39-4431-8031-b4e41d2d15b2"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#%cd course-deep-learning/\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020178c4-31a3-49b8-a2e6-d61e205f9d67",
   "metadata": {
    "id": "020178c4-31a3-49b8-a2e6-d61e205f9d67",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "## __2.__ <a name=\"data\">Data</a>\n",
    "\n",
    "### __2.1__ <a name=\"datasets\">Datasets</a>\n",
    "\n",
    "In the previous two notebooks, we saw a variety of two-dimensional synthetic datasets. In this notebook, we'll be working with a pre-existing image dataset. Image data is inherently high-dimensional: each pixel corresponds to a single coordinate/dimension (grayscale), or holds three separate coordinates (Red,Green,Blue). For even small images, this means our inputs can have thousands of dimensions (e.g. 32 x 32 pixels x 3 colors = 3072). As a result, image datasets can be fairly large. Additionally, we may need to apply certain __transformations__ or __preprocessing__ steps to our image data before attempting to pass it to a neural network.\n",
    "\n",
    "PyTorch and its corresponding image library, TorchVision, offer a number of utilities to streamline dataset storage, loading, and preprocessing. We'll start by using TorchVision to download the well-known [MNIST dataset](http://yann.lecun.com/exdb/mnist/). This dataset contains 28x28-pixel images of handwritten digits, and our goal will be to predict the correct label given an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "431fcf53-5a9b-468b-8287-e873c41d1fbb",
   "metadata": {
    "id": "431fcf53-5a9b-468b-8287-e873c41d1fbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb5786dbe3c473eab8bb502261b8aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2a71519d004a00a8e18116bdffa0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c684b950247f463a9250a2907a0d620f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce084c9676074cd3a231a4e113f18647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torchvision.datasets.mnist.MNIST,\n",
       " torch.utils.data.dataset.Subset,\n",
       " torch.utils.data.dataset.Subset)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a new directory in which to download the MNIST dataset\n",
    "data_dir = \"./data/\"\n",
    "\n",
    "# download MNIST \"test\" dataset\n",
    "mnist_test = torchvision.datasets.MNIST(data_dir, train=False, download=True)\n",
    "\n",
    "# download MNIST \"train\" dataset and set aside a portion for validation\n",
    "mnist_train_full = datasets.MNIST(data_dir, train=True, download=True)\n",
    "mnist_train, mnist_val = torch.utils.data.random_split(mnist_train_full, [55000, 5000])\n",
    "\n",
    "type(mnist_test), type(mnist_train), type(mnist_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cea4af4-c158-4ef1-a02a-bce0fba736d1",
   "metadata": {
    "id": "8cea4af4-c158-4ef1-a02a-bce0fba736d1",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "Our dataset is now held in three `torch.utils.data.Dataset` objects, each acting as an iterable container from which we can fetch input-label pairs. You should also now see a `data/` directory containing the MNIST dataset. Let's have a look at a random image from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87582752-ad1f-4c56-a5a2-a9d478a3ec17",
   "metadata": {
    "id": "87582752-ad1f-4c56-a5a2-a9d478a3ec17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10000 images in mnist_test\n",
      "Image 4346 is a 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"There are {len(mnist_test)} images in mnist_test\")\n",
    "d = np.random.randint(0, len(mnist_test))\n",
    "print(f\"Image {d} is a {mnist_test[d][1]}\")\n",
    "\n",
    "# plot our image\n",
    "plt.imshow(mnist_test[d][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80efbb56-c23f-43e7-9f99-01a28e376321",
   "metadata": {
    "id": "80efbb56-c23f-43e7-9f99-01a28e376321",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "Our \"test\" dataset contains 10,000 entries, each of which is a tuple holding a `PIL.Image.Image` object and an integer label. Unfortunately, the neural networks we trained in the previous notebook require `torch.Tensor` inputs. We therefore need to apply some preprocessing to these image datasets before we can train a network.\n",
    "\n",
    "TorchVision provides a `Transform` class for building and composing preprocessing stages that can be automatically applied to your image data. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1627880-672e-415a-a509-361722bd19e4",
   "metadata": {
    "id": "c1627880-672e-415a-a509-361722bd19e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image label: 7\n",
      "Transformed image shape: torch.Size([1, 28, 28])\n",
      "Transformed image data: 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  ...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we'll stack multiple transformations in a single object that will apply them in sequence\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),  # this is a built-in Transform object to convert images to tensors\n",
    "    lambda x: x>0,  # this is our own transformation function for binarizing MNIST images\n",
    "    lambda x: x.float(),  # this is our own transformation function for converting inputs to floating-point\n",
    "])\n",
    "\n",
    "# grab the first image-label pair from our \"test\" dataset\n",
    "example_img, example_label = mnist_test[0]\n",
    "\n",
    "# apply our sequence of transformations\n",
    "transformed = transform(example_img)\n",
    "print(f\"Image label: {example_label}\")\n",
    "print(\"Transformed image shape:\", transformed.shape)\n",
    "print(f\"Transformed image data: {(', '.join(str(p.item()) for p in transformed.flatten()))[:100]} ...\")\n",
    "\n",
    "# plot our image\n",
    "plt.imshow(transformed.squeeze(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ee41e-b55e-4e05-a30e-27adb99b7fba",
   "metadata": {
    "id": "e35ee41e-b55e-4e05-a30e-27adb99b7fba",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "We can see that our transform converts MNIST images to floating-point tensors holding binary values -- which we can feed to a neural network! In fact, we can bake our transform directly into our datasets so that it is applied automatically when we go to fetch data. To demonstrate, we'll re-initialize our datasets, this time reading directly from our `data/` folder rather than re-downloading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcf0ea06-2879-4ab3-80c7-0c5f05bdeaa2",
   "metadata": {
    "id": "bcf0ea06-2879-4ab3-80c7-0c5f05bdeaa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each image in our dataset now has type <class 'torch.Tensor'> and shape torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# load MNIST \"test\" dataset from disk. Note we're using the transform defined a few cells earlier, which\n",
    "# turns the data into the right format as we load from disk.\n",
    "mnist_test = torchvision.datasets.MNIST(data_dir, train=False, download=False, transform=transform)\n",
    "\n",
    "# load MNIST \"train\" dataset from disk and set aside a portion for validation\n",
    "mnist_train_full = datasets.MNIST(data_dir, train=True, download=False, transform=transform)\n",
    "mnist_train, mnist_val = torch.utils.data.random_split(mnist_train_full, [55000, 5000])\n",
    "\n",
    "example_img, example_label = mnist_test[0]\n",
    "print(f\"Each image in our dataset now has type {type(example_img)} and shape {example_img.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0bc892-84ee-4824-81a9-9144d9fe3825",
   "metadata": {
    "id": "5e0bc892-84ee-4824-81a9-9144d9fe3825",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "### __2.2__ <a name=\"dataloaders\">DataLoaders</a>\n",
    "\n",
    "Given that `torch.utils.data.Dataset` and its subclasses provide an iterable container from which we can fetch input-label pairs, we could go ahead and start traininng a network:\n",
    "\n",
    "```\n",
    "for x, y in myDataset:\n",
    "\n",
    "    opt.zero_grad()\n",
    "\n",
    "    outputs = myNetwork(x)\n",
    "    \n",
    "    loss = myLoss(outputs, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    opt.step()\n",
    "    ...\n",
    "```\n",
    "\n",
    "However, we often want to load our data in __batches__ while training, typically in a random or __shuffled__ order. PyTorch provides a `DataLoader` class to handle the process of fetching data from a `Dataset` object, including shuffling, custom batch collation, and various random sampling schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9f0e984-9e5c-4c1d-9bb5-cd11a7218531",
   "metadata": {
    "id": "a9f0e984-9e5c-4c1d-9bb5-cd11a7218531"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch inputs shape: torch.Size([60, 1, 28, 28]), Batch labels shape: torch.Size([60])\n"
     ]
    }
   ],
   "source": [
    "# we'll use a batch size of 60 for training our network\n",
    "batch_size = 60\n",
    "\n",
    "# initialize a DataLoader object for each dataset\n",
    "train_dataloader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(mnist_val, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(mnist_test, batch_size=1, shuffle=False)\n",
    "\n",
    "# grab the first batch from one of our DataLoader objects\n",
    "example_batch_img, example_batch_label = next(iter(train_dataloader))\n",
    "\n",
    "#for batch in train_dataloader:#\n",
    "\n",
    "#  print(batch[0], batch[1])\n",
    "#  break\n",
    "\n",
    "# inputs and labels are batched together as tensor objects\n",
    "print(f\"Batch inputs shape: {example_batch_img.shape}, Batch labels shape: {example_batch_label.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5717ae-f257-4b1c-bcfe-766742a570f7",
   "metadata": {
    "id": "aa5717ae-f257-4b1c-bcfe-766742a570f7",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "## __3.__ <a name=\"nn\">A Neural Network for Image Recognition</a>\n",
    "\n",
    "### __3.1__ <a name=\"definition\">Defining the Network</a>\n",
    "\n",
    "Now that we've seen the data we'll be working with, it's time build a neural network capable of classifying handwritten digits. In the previous notebook, we created a neural network capable of turning two-dimensional inputs into one-dimensional (scalar) predictions. By contrast, our inputs will have 28x28 = 784 dimensions, and our network will have to predict one of ten possible labels (one for each digit 0-9). To accommodate these changes, we'll tweak our network as follows:\n",
    "\n",
    "   1. We'll modify our network's first layer to take 784-dimensional inputs\n",
    "   2. We'll use a larger intermediate layer to allow our network to learn complex decision functions\n",
    "   3. We'll try out the ReLU (rectified linear unit) activation function \n",
    "   4. We'll have our network produce a 10-dimensional vector as output; the index of the largest value in this vector will be our predicted label (e.g. if the first entry has the largest value, our predicted digit will be 0).\n",
    "   \n",
    "<br/>\n",
    "<center>\n",
    "<img width=\"500px\" src=\"https://drive.google.com/uc?export=view&id=1fCIzQT6smKorQAfFmJp7GsGBC9atMo4U\"/>\n",
    "</center>\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f316de00-6063-40d8-b048-a3bca6a0789e",
   "metadata": {
    "id": "f316de00-6063-40d8-b048-a3bca6a0789e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNISTNetwork(\n",
       "  (layer_1): Linear(in_features=784, out_features=1024, bias=True)\n",
       "  (layer_2): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MNISTNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # MNIST images are (1, 28, 28) (channels, width, height)\n",
    "        self.layer_1 = torch.nn.Linear(28*28, 1024)\n",
    "        self.layer_2 = torch.nn.Linear(1024, 10)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size, channels, width, height = x.size()\n",
    "        x = x.view(batch_size, -1)  # create an array of flattened images with dimension (batch_size, num_pixels)\n",
    "        \n",
    "        # this time, we'll use the ReLU nonlinearity at each layer  \n",
    "        x = self.relu(self.layer_1(x))\n",
    "        x = self.layer_2(x)  # we'll avoid \"squashing\" our final outputs by omitting the sigmoid\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = MNISTNetwork()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f646c8-1f72-424e-a1c7-d6dd93ad3a00",
   "metadata": {
    "id": "a7f646c8-1f72-424e-a1c7-d6dd93ad3a00",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "### __3.2__ <a name=\"loss\">Classification Loss</a>\n",
    "\n",
    "In the previous notebook, we used mean squared error loss to train our neural network. While mean squared error performs well in a number of tasks, it is more common to use __categorical cross-entropy loss__ for multiclass classification. We can think of our network's output as a vector of ten \"class scores,\" one per digit. In training our network, our goal is to make sure that given an input image, the correct class score \"comes out on top.\" We might try to minimize the mean squared error between our network's normalized output and a __one-hot__ vector indexing the correct label\n",
    "\n",
    "```\n",
    "prediction = [0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.5, 0.1, 0.1]\n",
    "target =     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
    "```\n",
    "\n",
    "However, this objective does not necessarily correspond to our goal of maximizing the score of the target class while keeping all other scores low. Cross entropy loss generally does a better job of capturing this objective for multiclass classification, and its use can be considered equivalent to maximum-likelihood estimation under certain assumptions. We will use PyTorch's implementation, which provides an object capable of both computing the loss on pairs of tensors and computing gradients during the backward pass. We won't go into detail here, but for more info, check out the [official documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html). Below, we show an example of calculating loss for a bit of made-up data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a60994ee-a190-485a-b245-9c5a8faa3bcc",
   "metadata": {
    "id": "a60994ee-a190-485a-b245-9c5a8faa3bcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6965)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a PyTorch cross-entropy loss object\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# the loss object takes in a vector of class scores and a vector of target class indices\n",
    "preds = torch.randn(batch_size, 10)  # make a batch of random \"class score\" vectors, each with 10 scores corresponding to digits\n",
    "targets = torch.full((batch_size,), 7).long()  # make a batch of target indices; here, we'll set 7 as the target for all predictions\n",
    "\n",
    "# compute the loss for this batch; by default, CrossEntropyLoss will average over a batch to return a scalar\n",
    "loss_fn(preds, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbe872c-fd79-4a7d-9c39-7ccbc9d81517",
   "metadata": {
    "id": "fcbe872c-fd79-4a7d-9c39-7ccbc9d81517",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "### __3.3__ <a name=\"sgd\">Picking an Optimizer: SGD</a>\n",
    "\n",
    "Recall that each training iteration can be broken down as follows: \n",
    "* we pass inputs to our network and collect outputs\n",
    "* we compute a differentiable a scalar loss on our network's outputs\n",
    "* we use backpropagation to compute the gradients of the loss with respect to our network's weights\n",
    "* we perform a gradient-based update on our weights to reduce the loss \n",
    "\n",
    "In the previous notebook, we made use of a built-in __optimizer__ to automate the process of updating our network's weights. This optimizer object stores references to our network's weights. When our backpropagation step (`backward()`) computes and stores gradients for all network parameters, the optimizer fetches these gradients and performs an update determined by its optimization algorithm. When training neural networks with large numbers of parameters, this becomes much simpler than manually updating each weight.\n",
    "\n",
    "PyTorch offers a number of [optimization algorithms](https://pytorch.org/docs/stable/optim.html), all of which use the same basic interface:\n",
    "\n",
    "```\n",
    "optimizer = OptimizerName(my_model.parameters(), lr=my_learning_rate, *other_params)\n",
    "```\n",
    "\n",
    "Each optimizer requires an iterable containing our network's weights (which the `.parameters()` method of any `torch.nn.Module` object provides) and a __learning rate__. As in the last notebook, we'll use __Stochastic Gradient Descent (SGD)__ to determine our updates. This algorithm scales the computed gradients with its learning rate and subtracts them from their respective weights to \"descend\" the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7a89209-9e7f-4b37-b64f-8efcae8fcece",
   "metadata": {
    "id": "c7a89209-9e7f-4b37-b64f-8efcae8fcece"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting weights: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), Sum: 0.0\n",
      "Updated weights: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), Sum: 10.0\n"
     ]
    }
   ],
   "source": [
    "# a simple optimization problem: we want our \"weights\" to sum to 10\n",
    "weights = torch.zeros(10).requires_grad_(True)\n",
    "print(f\"Starting weights: {weights}, Sum: {weights.sum().item()}\")\n",
    "\n",
    "# create an optimizer object and pass it an Iterable containing our \"weights\".\n",
    "# In this example, we'll take steps of size 1.0, meaning that each weight will \n",
    "# change by an amount equal to the magnitude of its gradient\n",
    "opt = torch.optim.SGD([weights], lr = 1.0)  \n",
    "\n",
    "# compute loss and perform backpropagation\n",
    "loss = 10 - weights.sum()\n",
    "loss.backward()\n",
    "\n",
    "# perform an optimization step, i.e. a gradient-based update of our weights\n",
    "opt.step()\n",
    "\n",
    "print(f\"Updated weights: {weights}, Sum: {weights.sum().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e772d67a-67c8-4d88-970d-87fefadd65bb",
   "metadata": {
    "id": "e772d67a-67c8-4d88-970d-87fefadd65bb",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "### __3.4__ <a name=\"checkpoint\">Checkpointing</a>\n",
    "\n",
    "Before we begin training our model, we want to make sure we can save it in some format in case we experience a bug during training or want to use it again later. The process of saving snapshots of a model during training is often called __checkpointing__, and PyTorch offers utilities to make saving and loading models simple. For a neural network, saving a model really means saving its weights (parameters). All PyTorch models have a `.state_dict()` method that exposes their weights as named entries in a dictionary. Using this __state dictionary__, we can easily save weights or overwrite them with ones we load from elsewhere. For more info, feel free to check out the [official documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a55bb3e5-593f-491d-b804-362342febb3c",
   "metadata": {
    "id": "a55bb3e5-593f-491d-b804-362342febb3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names of network weights: ['layer_1.weight', 'layer_1.bias', 'layer_2.weight', 'layer_2.bias']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MNISTNetwork(\n",
       "  (layer_1): Linear(in_features=784, out_features=1024, bias=True)\n",
       "  (layer_2): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a model\n",
    "model = MNISTNetwork()\n",
    "print(\"Names of network weights:\", list(model.state_dict().keys()))\n",
    "\n",
    "# save weights to disk\n",
    "torch.save(model.state_dict(), \"dummy_weights.pt\")\n",
    "\n",
    "# load weights from disk and overwrite network weights\n",
    "model.load_state_dict(torch.load(\"dummy_weights.pt\"))\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc6805-b78c-4005-96f8-a192ba040769",
   "metadata": {
    "id": "ccfc6805-b78c-4005-96f8-a192ba040769",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "## __4.__ <a name=\"train\">Putting It All Together: Training Loop</a>\n",
    "We're now ready to train a neural network to recognize handwritten digits from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44658ffa-8a06-401e-b8a2-7429c20da354",
   "metadata": {
    "id": "44658ffa-8a06-401e-b8a2-7429c20da354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: val loss 0.471, val acc 0.880, train loss 0.971, train acc 0.794\n",
      "New best accuracy; saving model weights to mnist_basic.pt\n",
      "Epoch 2: val loss 0.360, val acc 0.897, train loss 0.419, train acc 0.886\n",
      "New best accuracy; saving model weights to mnist_basic.pt\n",
      "Epoch 3: val loss 0.318, val acc 0.905, train loss 0.353, train acc 0.899\n",
      "New best accuracy; saving model weights to mnist_basic.pt\n",
      "Epoch 4: val loss 0.296, val acc 0.908, train loss 0.320, train acc 0.908\n",
      "New best accuracy; saving model weights to mnist_basic.pt\n",
      "Epoch 5: val loss 0.276, val acc 0.916, train loss 0.296, train acc 0.914\n",
      "New best accuracy; saving model weights to mnist_basic.pt\n",
      "Epoch 6: val loss 0.262, val acc 0.921, train loss 0.278, train acc 0.920\n",
      "New best accuracy; saving model weights to mnist_basic.pt\n",
      "Epoch 7: val loss 0.249, val acc 0.924, train loss 0.262, train acc 0.924\n",
      "New best accuracy; saving model weights to mnist_basic.pt\n",
      "Epoch 8: val loss 0.237, val acc 0.929, train loss 0.247, train acc 0.929\n",
      "New best accuracy; saving model weights to mnist_basic.pt\n",
      "Epoch 9: val loss 0.226, val acc 0.929, train loss 0.235, train acc 0.932\n",
      "New best accuracy; saving model weights to mnist_basic.pt\n",
      "Epoch 10: val loss 0.217, val acc 0.933, train loss 0.224, train acc 0.935\n",
      "New best accuracy; saving model weights to mnist_basic.pt\n",
      "Total training time (s): 132.480\n"
     ]
    }
   ],
   "source": [
    "def training_loop(save_path, epochs, batch_size, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Train a neural network model for digit recognition on the MNIST dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    save_path (str):  path/filename for model checkpoint, e.g. 'my_model.pt'\n",
    "    \n",
    "    epochs (int):     number of iterations through the whole dataset for training\n",
    "    \n",
    "    batch_size (int): size of a single batch of inputs\n",
    "    \n",
    "    device (str):     device on which tensors are placed; should be 'cpu' or 'cuda'. \n",
    "                      More on this in the next section!\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model (nn.Module): final trained model\n",
    "    \n",
    "    save_path (str):   path/filename for model checkpoint, so that we can load our model\n",
    "                       later to test on unseen data\n",
    "    \n",
    "    device (str):      the device on which we carried out training, so we can match it\n",
    "                       when we test the final model on unseen data later\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize model\n",
    "    model = MNISTNetwork()\n",
    "    model.to(device)  # we'll cover this in the next section!\n",
    "\n",
    "    # initialize an optimizer to update our model's parameters during training\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # make a new directory in which to download the MNIST dataset\n",
    "    data_dir = \"./data/\"\n",
    "    \n",
    "    # initialize a Transform object to prepare our data\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        lambda x: x>0,\n",
    "        lambda x: x.float(),\n",
    "    ])\n",
    "\n",
    "    # load MNIST \"test\" dataset from disk\n",
    "    mnist_test = torchvision.datasets.MNIST(data_dir, train=False, download=False, transform=transform)\n",
    "\n",
    "    # load MNIST \"train\" dataset from disk and set aside a portion for validation\n",
    "    mnist_train_full = datasets.MNIST(data_dir, train=True, download=False, transform=transform)\n",
    "    mnist_train, mnist_val = torch.utils.data.random_split(mnist_train_full, [55000, 5000])\n",
    "\n",
    "    # initialize a DataLoader object for each dataset\n",
    "    train_dataloader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(mnist_val, batch_size=batch_size, shuffle=False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(mnist_test, batch_size=1, shuffle=False)\n",
    "\n",
    "    # a PyTorch categorical cross-entropy loss object\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # time training process\n",
    "    st = time.time()\n",
    "\n",
    "    # time to start training!\n",
    "    for epoch_idx, epoch in enumerate(range(epochs)):\n",
    "\n",
    "        # keep track of best validation accuracy; if improved upon, save checkpoint\n",
    "        best_acc = 0.0\n",
    "\n",
    "        # loop through the entire dataset once per epoch\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        train_total = 0\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # unpack data and labels\n",
    "            x, y = batch\n",
    "            x = x.to(device)  # we'll cover this in the next section!\n",
    "            y = y.to(device)  # we'll cover this in the next section!\n",
    "\n",
    "            # generate predictions and compute loss\n",
    "            output = model(x)  # (batch_size, 10)\n",
    "            loss = loss_fn(output, y)\n",
    "\n",
    "            # compute accuracy\n",
    "            preds = output.argmax(dim=1)\n",
    "            acc = preds.eq(y).sum().item()/len(y)\n",
    "\n",
    "            # compute gradients and update model parameters\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update statistics\n",
    "            train_loss += (loss * len(x))\n",
    "            train_acc += (acc * len(x))\n",
    "            train_total += len(x)\n",
    "\n",
    "        train_loss /= train_total\n",
    "        train_acc /= train_total\n",
    "\n",
    "        # perform validation once per epoch\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        val_total = 0\n",
    "        model.eval()\n",
    "        for batch_idx, batch in enumerate(val_dataloader):\n",
    "\n",
    "            # don't compute gradients during validation\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # unpack data and labels\n",
    "                x, y = batch\n",
    "                x = x.to(device)  # we'll cover this in the next section!\n",
    "                y = y.to(device)  # we'll cover this in the next section!\n",
    "\n",
    "                # generate predictions and compute loss\n",
    "                output = model(x)\n",
    "                loss = loss_fn(output, y)\n",
    "\n",
    "                # compute accuracy\n",
    "                preds = output.argmax(dim=1)\n",
    "                acc = preds.eq(y).sum().item()/len(y)\n",
    "\n",
    "                # update statistics\n",
    "                val_loss += (loss * len(x))\n",
    "                val_acc += (acc * len(x))\n",
    "                val_total += len(x)\n",
    "\n",
    "        val_loss /= val_total\n",
    "        val_acc /= val_total\n",
    "        print(f\"Epoch {epoch_idx + 1}: val loss {val_loss :0.3f}, val acc {val_acc :0.3f}, train loss {train_loss :0.3f}, train acc {train_acc :0.3f}\")\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "\n",
    "            best_acc = val_acc\n",
    "            print(f\"New best accuracy; saving model weights to {save_path}\")\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    print(f\"Total training time (s): {time.time() - st :0.3f}\")\n",
    "    \n",
    "    return model, save_path, device\n",
    "\n",
    "    \n",
    "# run our training loop\n",
    "model, save_path, device = training_loop(\"mnist_basic.pt\", 10, 60, \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bFe7Zw54DEyr",
   "metadata": {
    "id": "bFe7Zw54DEyr"
   },
   "source": [
    "Once we're done training, we now load the best saved version of the model weights (which may not be the one from the final epoch) and compute final performance on unseen test data. Typically, this is reserved for after the model development process, so we get an unbiased estimate of the model's generalized accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "961472ee-b038-4ae0-bd93-2becc8f653dc",
   "metadata": {
    "id": "961472ee-b038-4ae0-bd93-2becc8f653dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss 0.217, test acc 0.937\n"
     ]
    }
   ],
   "source": [
    "# load best weights\n",
    "model.load_state_dict(torch.load(save_path, map_location=device))\n",
    "\n",
    "test_loss = 0.0\n",
    "test_acc = 0.0\n",
    "test_total = 0\n",
    "model.eval()\n",
    "for batch_idx, batch in enumerate(test_dataloader):\n",
    "\n",
    "    # don't compute gradients during validation\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # unpack data and labels\n",
    "        x, y = batch\n",
    "        x = x.to(device)  # we'll cover this in the next section!\n",
    "        y = y.to(device)  # we'll cover this in the next section!\n",
    "\n",
    "        # generate predictions and compute loss\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output, y)\n",
    "\n",
    "        # compute accuracy\n",
    "        preds = output.argmax(dim=1)\n",
    "        acc = preds.eq(y).sum().item()/len(y)\n",
    "\n",
    "        # update statistics\n",
    "        test_loss += (loss * len(x))\n",
    "        test_acc += (acc * len(x))\n",
    "        test_total += len(x)\n",
    "\n",
    "test_loss /= test_total\n",
    "test_acc /= test_total\n",
    "print(f\"test loss {test_loss :0.3f}, test acc {test_acc :0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd262fc-3bf6-4a57-8ad6-7372cda4bded",
   "metadata": {
    "id": "edd262fc-3bf6-4a57-8ad6-7372cda4bded",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e",
    "tags": []
   },
   "source": [
    "## __5.__ <a name=\"gpu\">GPU Acceleration</a>\n",
    "\n",
    "You might have noticed all the mentions of a `device` in the cells above. It turns out that neural networks use many operations, such as matrix multiplication, that can be efficiently parallelized and run on modern GPUs (graphics processing units, sometimes called \"video cards\"). As a result, neural network training and inference can see drastic speedups when run on a suitable GPU. PyTorch offers this option for NVIDIA-manufactured GPUs through the [CUDA platform](https://pytorch.org/docs/stable/cuda.html), and provides a simple interface (`.to()`) for moving data and computation between the CPU and GPU devices. To move data to the CPU, we can call:\n",
    "\n",
    "```\n",
    "x = x.to(\"cpu\")\n",
    "```\n",
    "\n",
    "To move data to a compatible NVIDIA GPU, we can call:\n",
    "\n",
    "```\n",
    "x = x.to(\"cuda\")\n",
    "```\n",
    "\n",
    "In practice, running machine learning code on a GPU may require you to check your device's compatibility and install various drivers; this can be quite a hassle. Luckily, [Google Colab](https://colab.research.google.com/) provides free (albeit limited) access to GPUs in a Jupyter-like notebook environment. If you're already running this code in Colab, you can access a GPU by going to `Runtime` > `Change runtime type`, setting `Hardware accelerator` to `GPU`, and clicking `Save`. Note that this will restart the notebook, meaning you will have to run your code again.\n",
    "\n",
    "Below, we'll try our basic training loop again. This time, however, we'll move our network and data to the GPU, allowing for faster training and inference. While the difference between CPU and GPU may be relatively minor in this case, it can be massive for larger models and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d12RFSDR6Oej",
   "metadata": {
    "id": "d12RFSDR6Oej"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\r\n"
     ]
    }
   ],
   "source": [
    "# run this terminal command to see the details of your Colab server's GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "706c9c65-096d-4e30-8e3b-f9dc44b26cd4",
   "metadata": {
    "id": "706c9c65-096d-4e30-8e3b-f9dc44b26cd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compatible GPU found; your code will run on the CPU again\n",
      "Epoch 1: val loss 0.487, val acc 0.869, train loss 0.960, train acc 0.795\n",
      "New best accuracy; saving model weights to mnist_gpu.pt\n",
      "Epoch 2: val loss 0.385, val acc 0.888, train loss 0.417, train acc 0.886\n",
      "New best accuracy; saving model weights to mnist_gpu.pt\n",
      "Epoch 3: val loss 0.346, val acc 0.900, train loss 0.352, train acc 0.899\n",
      "New best accuracy; saving model weights to mnist_gpu.pt\n",
      "Epoch 4: val loss 0.323, val acc 0.909, train loss 0.321, train acc 0.907\n",
      "New best accuracy; saving model weights to mnist_gpu.pt\n",
      "Epoch 5: val loss 0.304, val acc 0.910, train loss 0.298, train acc 0.913\n",
      "New best accuracy; saving model weights to mnist_gpu.pt\n",
      "Epoch 6: val loss 0.288, val acc 0.917, train loss 0.279, train acc 0.919\n",
      "New best accuracy; saving model weights to mnist_gpu.pt\n",
      "Epoch 7: val loss 0.276, val acc 0.920, train loss 0.263, train acc 0.924\n",
      "New best accuracy; saving model weights to mnist_gpu.pt\n",
      "Epoch 8: val loss 0.264, val acc 0.923, train loss 0.249, train acc 0.927\n",
      "New best accuracy; saving model weights to mnist_gpu.pt\n",
      "Epoch 9: val loss 0.253, val acc 0.926, train loss 0.236, train acc 0.931\n",
      "New best accuracy; saving model weights to mnist_gpu.pt\n",
      "Epoch 10: val loss 0.243, val acc 0.929, train loss 0.225, train acc 0.935\n",
      "New best accuracy; saving model weights to mnist_gpu.pt\n",
      "Total training time (s): 121.335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(MNISTNetwork(\n",
       "   (layer_1): Linear(in_features=784, out_features=1024, bias=True)\n",
       "   (layer_2): Linear(in_features=1024, out_features=10, bias=True)\n",
       "   (relu): ReLU()\n",
       " ),\n",
       " 'mnist_gpu.pt',\n",
       " device(type='cpu'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, let's check if we can access a compatible GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Found a CUDA-compatible GPU!\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"No compatible GPU found; your code will run on the CPU again\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "training_loop(\"mnist_gpu.pt\", 10, 60, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f8e68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_3_image_classification (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
